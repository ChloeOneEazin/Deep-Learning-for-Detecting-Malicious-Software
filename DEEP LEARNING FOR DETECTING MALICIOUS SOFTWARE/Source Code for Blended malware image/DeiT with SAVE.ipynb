{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac300b5",
   "metadata": {},
   "source": [
    "**DeiT with SAVE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbc7b50",
   "metadata": {},
   "source": [
    "Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf380e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense, Layer, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from vit_keras import vit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f880717",
   "metadata": {},
   "source": [
    "Dataset Configuration and Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e7c2cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = \"Blended malware image\"\n",
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "classes = sorted(\n",
    "    d for d in os.listdir(source_dir)\n",
    "    if os.path.isdir(os.path.join(source_dir, d))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6829bc81",
   "metadata": {},
   "source": [
    "Gather Files Path and Integer Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2704d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths, labels = [], []\n",
    "for label, cls in enumerate(classes):\n",
    "    cls_dir = os.path.join(source_dir, cls)\n",
    "    for fname in os.listdir(cls_dir):\n",
    "        fpath = os.path.join(cls_dir, fname)\n",
    "        if os.path.isfile(fpath):\n",
    "            file_paths.append(fpath)\n",
    "            labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a751081",
   "metadata": {},
   "source": [
    "Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "820c539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(file_paths, labels, test_size=0.30, stratify=labels, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc526619",
   "metadata": {},
   "source": [
    "TF-Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a41ea66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function process_path at 0x000002401DAF85E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function process_path at 0x000002401DAF85E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "def process_path(file_path, label):\n",
    "    img = tf.io.decode_jpeg(tf.io.read_file(file_path), channels=3)\n",
    "    img = tf.image.resize(img, image_size)\n",
    "    img = tf.keras.applications.imagenet_utils.preprocess_input(img)\n",
    "    label = tf.one_hot(label, depth=len(classes))\n",
    "    return img, label\n",
    "\n",
    "def make_dataset(X, y, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(X))\n",
    "    return (\n",
    "        ds.\n",
    "        map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "train_ds = make_dataset(X_train, y_train, shuffle=True)\n",
    "val_ds = make_dataset(X_val, y_val, shuffle=False)\n",
    "test_ds = make_dataset(X_test, y_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d27f614",
   "metadata": {},
   "source": [
    "Spatial Aggregation Vector Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "319b1499",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAggregationLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.alpha_top    = self.add_weight('alpha_top',    (1,), initializer='ones')\n",
    "        self.alpha_bottom = self.add_weight('alpha_bottom', (1,), initializer='ones')\n",
    "        self.alpha_left   = self.add_weight('alpha_left',   (1,), initializer='ones')\n",
    "        self.alpha_right  = self.add_weight('alpha_right',  (1,), initializer='ones')\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        t = tf.roll(inputs, -1, axis=1)\n",
    "        b = tf.roll(inputs,  1, axis=1)\n",
    "        l = tf.roll(inputs, -1, axis=2)\n",
    "        r = tf.roll(inputs,  1, axis=2)\n",
    "        return inputs \\\n",
    "            + self.alpha_top    * t \\\n",
    "            + self.alpha_bottom * b \\\n",
    "            + self.alpha_left   * l \\\n",
    "            + self.alpha_right  * r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334188a8",
   "metadata": {},
   "source": [
    "DeiT Model with SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a59e04f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\vit_keras\\utils.py:83: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method SpatialAggregationLayer.call of <__main__.SpatialAggregationLayer object at 0x000002401DFCB488>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method SpatialAggregationLayer.call of <__main__.SpatialAggregationLayer object at 0x000002401DFCB488>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " embedding (Conv2D)          (None, 14, 14, 768)       590592    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 196, 768)          0         \n",
      "                                                                 \n",
      " class_token (ClassToken)    (None, 197, 768)          768       \n",
      "                                                                 \n",
      " Transformer/posembed_input   (None, 197, 768)         151296    \n",
      " (AddPositionEmbs)                                               \n",
      "                                                                 \n",
      " Transformer/encoderblock_0   ((None, 197, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_1   ((None, 197, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_2   ((None, 197, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_3   ((None, 197, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_4   ((None, 197, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_5   ((None, 197, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_6   ((None, 197, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_7   ((None, 197, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_8   ((None, 197, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_9   ((None, 197, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_10  ((None, 197, 768),       7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_11  ((None, 197, 768),       7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoder_norm (L  (None, 197, 768)         1536      \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " fold_patches (Lambda)       (None, 14, 14, 768)       0         \n",
      "                                                                 \n",
      " spatial_aggregation_layer (  (None, 14, 14, 768)      4         \n",
      " SpatialAggregationLayer)                                        \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 150528)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              150529000 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 31)                31031     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 236,358,691\n",
      "Trainable params: 236,358,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_deit_model(num_classes, image_size=image_size, patch_size=16):\n",
    "    backbone = vit.vit_b16(\n",
    "        image_size=image_size,\n",
    "        pretrained=True,\n",
    "        include_top=False,\n",
    "        pretrained_top=False\n",
    "    )\n",
    "    seq_output = backbone.get_layer(\"Transformer/encoder_norm\").output\n",
    "    seq_model  = Model(inputs=backbone.input, outputs=seq_output)\n",
    "\n",
    "    def fold_patches(x):\n",
    "        patch_tokens = x[:, 1:, :]\n",
    "        return tf.reshape(\n",
    "            patch_tokens,\n",
    "            (-1,\n",
    "             image_size[0]//patch_size,\n",
    "             image_size[1]//patch_size,\n",
    "             x.shape[-1])\n",
    "        )\n",
    "\n",
    "    x = Lambda(fold_patches, name=\"fold_patches\")(seq_model.output)\n",
    "    x = SpatialAggregationLayer()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1000, activation=\"relu\")(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=seq_model.input, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.AUC(name='auc'),\n",
    "            tf.keras.metrics.FalsePositives(name='false_positives'),\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_deit_model(num_classes=len(classes))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6104f15e",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcde4c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002401DF77288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002401DF77288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "284/300 [===========================>..] - ETA: 5:58 - loss: 17.8900 - accuracy: 0.8545 - auc: 0.9370 - false_positives: 1292.0000 - precision: 0.8571 - recall: 0.8540"
     ]
    }
   ],
   "source": [
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath='checkpoints/vit_malware_best.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "train_ds_inf = make_dataset(X_train, y_train, shuffle=True) \\\n",
    "                   .repeat()  \n",
    "\n",
    "history = model.fit(\n",
    "    train_ds_inf,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=80,      \n",
    "    validation_data=val_ds,\n",
    "    callbacks=[checkpoint_cb]\n",
    ")\n",
    "\n",
    "model.save('models/vit_malware_final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43caf1d3",
   "metadata": {},
   "source": [
    "Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171fa169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(hist):\n",
    "    metrics = ['loss', 'accuracy', 'auc', 'false_positives', 'precision', 'recall']\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(12, 12))\n",
    "    for idx, m in enumerate(metrics):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        ax.plot(hist.history[m], label='train')\n",
    "        ax.plot(hist.history[f'val_{m}'], label='val')\n",
    "        ax.set_title(m)\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fa6e77",
   "metadata": {},
   "source": [
    "Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2321e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics = model.evaluate(val_ds, verbose=1)\n",
    "print(\"\\nValidation set metrics:\")\n",
    "for name, value in zip(model.metrics_names, val_metrics):\n",
    "    print(f\" - {name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3366b0a1",
   "metadata": {},
   "source": [
    "Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04ee21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "preds = model.predict(test_ds)\n",
    "print(f\"\\nInference time on test set: {time.time() - start:.2f}s\")\n",
    "\n",
    "y_true = np.argmax(\n",
    "    np.vstack([y for _, y in test_ds.unbatch()]), axis=1\n",
    ")\n",
    "y_pred = np.argmax(preds, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74efec9",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de361ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('Confusion Matrix Heatmap')\n",
    "plt.imshow(cm, interpolation='nearest', aspect='auto')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# annotate each cell\n",
    "thresh = cm.max() / 2\n",
    "for i, j in np.ndindex(cm.shape):\n",
    "    plt.text(\n",
    "        j, i, f\"{cm[i, j]:d}\",\n",
    "        ha='center', va='center',\n",
    "        color='white' if cm[i, j] > thresh else 'black'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
